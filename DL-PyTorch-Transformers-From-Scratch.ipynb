{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elmog\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\elmog\\AppData\\Local\\Temp\\ipykernel_17464\\1517835734.py:21: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.stats as stats\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#for importing data\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import pandas as pd\n",
    "\n",
    "#for data visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use embedding from scratch \"Embedding is learnable through training\"\n",
    "# or use word2vec pretrained model\n",
    "# word2vec will give better results!\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, vocab_length: int):\n",
    "        super().__init__()\n",
    "        self.d_model        = d_model\n",
    "        self.vocab_sequence = vocab_length\n",
    "        self.embedding      = nn.Embedding(\n",
    "            num_embeddings  = vocab_length, \n",
    "            embedding_dim   = d_model\n",
    "            )\n",
    "     \n",
    "    def forward(self, x):\n",
    "        # scaling the embedding vectors by multiplying them with the square root of d_model. Why? \n",
    "        # to maintain the variance of the embeddings when they are passed through the network\n",
    "        # To stabilize the gradients during training. \n",
    "        # It helps to prevent the gradients from becoming too small or too large, \n",
    "        # which can lead to issues like vanishing or exploding gradients.\n",
    "        \n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        return self.embedding(x) * Math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use positional encoding because the attention mechanisms are position invariant\n",
    "# it gives our model a way to understand where each word or token is in a sequence.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, sequence_len: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.d_model        = d_model\n",
    "        self.sequence_len   = sequence_len\n",
    "        self.dropout        = nn.Dropout(dropout) \n",
    "        \n",
    "        # batch size of 32 \n",
    "        # seq length of 10 \n",
    "        # embedding dimension be 512. \n",
    "        # Then we will have embedding vector of dimension 32 x 10 x 512\n",
    "        \n",
    "        # pe shape(sequence_length x d_model)\n",
    "        pe = torch.zeros(self.sequence_len, self.d_model)\n",
    "        \n",
    "        # NUMERATOR: position vector shape(sequence_len -> sequence_len x 1)\n",
    "        # For each token(word) in sequence, we will find the embedding vector which is of dimension 1 x 512 \n",
    "        # and it is added with the correspondng positional vector which is of dimension 1 x 512 \n",
    "        # to get 1 x 512 dim out for each word/token.\n",
    "        position = torch.arange(start=0, end=sequence_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Denomenator: div_term of the formula shape(d_model)\n",
    "        # calculation is done in log space for numerical stability\n",
    "        div_term = torch.exp(torch.arange(start=0, end=d_model, step=2).float() * (-Math.log(10000.0) / d_model))\n",
    "        # arange = 2i \n",
    "        \n",
    "        # Apply sine and cosine. The sine is applied to even numbers; cosine to odd numbers\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add a batch dimension to the positional encoding\n",
    "        pe = pe.unsqueeze(0) # (1, sequence_len, d_model)\n",
    "        \n",
    "        # Tensor that I want to keep in the model\n",
    "        # Not as a parameter\n",
    "        # I want it to be saved when I saved the state of the model \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # We slice it to match the dimensions of the word embedding\n",
    "        # We take dim 1 (pos_embeddings) and align them with the dim 1 of x (actual word embeddings)\n",
    "        # x.shape       = [batch_size=2,sequence_length=5,         embedding_dim=512]\n",
    "        # self.pe.shape = [             max_sequence_length=1000,  embedding_dim=512]\n",
    "        # self.pe[:, :x.shape[1]]  # Extracts the first 5 positional encodings\n",
    "        # take the needed number of sequences instead of all sequences\n",
    "        x = x + (self.pe[:, :x.shape[1]:]).requires_grad(False)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
